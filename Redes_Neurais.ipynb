{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Redes_Neurais.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNcg31GadOr3rjEw1twEk/y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/santanaemmanuel/redes_neurais_CESAR_SCHOOL/blob/main/Redes_Neurais.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preprocessing\n",
        "Esta primeira etapa tem como objetivo realizar o download do dataset e prepará-lo em um grupo de treino e outro de testes."
      ],
      "metadata": {
        "id": "PmkNdSIgT62O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "#O dataset a ser utilizado foi carregado no github\n",
        "!wget --no-check-certificate \\\n",
        "    \"https://github.com/santanaemmanuel/redes_neurais_CESAR_SCHOOL/archive/refs/heads/main.zip\" \\\n",
        "    -O \"/content/mtg-cards.zip\"\n",
        "\n",
        "#Realiza o dowload e a descompactação do dataset a ser utilizado\n",
        "zip_ref = zipfile.ZipFile('/content/mtg-cards.zip', 'r') #Opens the zip file in read mode\n",
        "zip_ref.extractall('/content') #Extracts the files into the /content folder\n",
        "zip_ref.close()\n"
      ],
      "metadata": {
        "id": "G8a-IqehbW-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/vcasadei/cats-and-dogs.git"
      ],
      "metadata": {
        "id": "y_AMSIVGpJTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uma vez que o dataset foi carregado no notebook, o próximo passo será dividí-lo em grupos de treino e teste"
      ],
      "metadata": {
        "id": "Ov5jf0yscgAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import re\n",
        "\n",
        "dataset_dir = \"/content/redes_neurais_CESAR_SCHOOL-main/dataset/mtg_cards\"\n",
        "base_dir = \"mtg-cards\"\n",
        "\n",
        "\n",
        "# Moves all training cat images to cats folder, training dog images to dogs folder\n",
        "def val_train_maker(name, val_size):\n",
        "  try:\n",
        "      path_train = f\"{base_dir}/train/{name}\"\n",
        "      path_val = f\"{base_dir}/val/{name}\"\n",
        "      os.makedirs(path_train, exist_ok=True)\n",
        "      os.makedirs(path_val, exist_ok=True)\n",
        "  except OSError:\n",
        "      print (\"Creation of the directory failed\")\n",
        "  else:\n",
        "      print (\"Successfully created the directories\")\n",
        "  files_path = os.path.join(dataset_dir, name)\n",
        "  files = os.listdir(files_path)\n",
        "  file_count = 0\n",
        "  for f in files:\n",
        "    if file_count < val_size:\n",
        "      shutil.move(f'{files_path}/{f}', path_train)\n",
        "      file_count += 1\n",
        "    else:\n",
        "      shutil.move(f'{files_path}/{f}', path_val)\n",
        "\n",
        "class_names = ['W','B','R','G','U']\n",
        "for name in class_names:\n",
        "   val_train_maker(name, 120)"
      ],
      "metadata": {
        "id": "JO9NwUSJdB2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuV1_FCETrT9"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make transforms and use data loaders\n",
        "\n",
        "# We'll use these a lot, so make them variables\n",
        "mean_nums = [0.485, 0.456, 0.406]\n",
        "std_nums = [0.229, 0.224, 0.225]\n",
        "\n",
        "chosen_transforms = {'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(size=256),\n",
        "        transforms.RandomRotation(degrees=15),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean_nums, std_nums)\n",
        "]), 'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean_nums, std_nums)\n",
        "]),\n",
        "}"
      ],
      "metadata": {
        "id": "ikiFEf4HYOw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the directory for the data\n",
        "data_dir = 'PetImages/'\n",
        "\n",
        "# Use the image folder function to create datasets\n",
        "chosen_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), chosen_transforms[x]) for x in ['train', 'val']}"
      ],
      "metadata": {
        "id": "jZyOR0e9YRAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make iterables with the dataloaders\n",
        "dataloaders = {x: torch.utils.data.DataLoader(chosen_datasets[x], batch_size=4,\n",
        "  shuffle=True, num_workers=4)\n",
        "              for x in ['train', 'val']}"
      ],
      "metadata": {
        "id": "X1FtbEvYYVtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_sizes = {x: len(chosen_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = chosen_datasets['train'].classes\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "rd-huQLsYY77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset_sizes)\n",
        "print(class_names)"
      ],
      "metadata": {
        "id": "FdCmeCTdYZjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Modelo\n",
        "O modelo resnet utilizado será o não treinado."
      ],
      "metadata": {
        "id": "_YKFSx57YgXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the model\n",
        "# load in pretrained and reset final fully connected\n",
        "\n",
        "res_mod = models.resnet34(pretrained=False)\n",
        "\n",
        "num_ftrs = res_mod.fc.in_features\n",
        "res_mod.fc = nn.Linear(num_ftrs, 2)"
      ],
      "metadata": {
        "id": "XVhdnthWYb_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_mod = res_mod.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(res_mod.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ],
      "metadata": {
        "id": "KdKUGBmvZFn5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}