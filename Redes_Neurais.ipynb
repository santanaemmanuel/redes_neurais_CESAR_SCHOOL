{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Redes_Neurais.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNcwX5Ep8BEHDP8zEPSRoXB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/santanaemmanuel/redes_neurais_CESAR_SCHOOL/blob/main/Redes_Neurais.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aluno: Emmanuel Dantas Ribeiro de Santana"
      ],
      "metadata": {
        "id": "vaaSb4rhVz1o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preprocessing\n",
        "Esta primeira etapa tem como objetivo realizar o download do dataset e prepará-lo em um grupo de treino e outro de testes. O conjunto de dados encontra-se no link do github a ser clonado"
      ],
      "metadata": {
        "id": "PmkNdSIgT62O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/santanaemmanuel/mtg-cards.git"
      ],
      "metadata": {
        "id": "y_AMSIVGpJTL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dc2dda5-0d5b-446a-b165-99ef5cb22323"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'mtg-cards' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uma vez que o dataset foi carregado no notebook, o próximo passo será dividí-lo em grupos de treino e teste. Cada categoria possui 600 imagens, dais quais 120 foram movidas para a pasta de validação e 480 para treino."
      ],
      "metadata": {
        "id": "Ov5jf0yscgAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import re\n",
        "\n",
        "dataset_dir = \"/content/mtg-cards\"\n",
        "base_dir = \"mtg_dataset\"\n",
        "\n",
        "# Moves all training cat images to cats folder, training dog images to dogs folder\n",
        "def val_train_maker(name, val_size):\n",
        "  try:\n",
        "      path_train = f\"{base_dir}/train/{name}\"\n",
        "      path_val = f\"{base_dir}/val/{name}\"\n",
        "      os.makedirs(path_train, exist_ok=True)\n",
        "      os.makedirs(path_val, exist_ok=True)\n",
        "  except OSError:\n",
        "      print (\"Creation of the directory failed\")\n",
        "  else:\n",
        "      print (\"Successfully created the directories\")\n",
        "  files_path = os.path.join(dataset_dir, name)\n",
        "  files = os.listdir(files_path)\n",
        "  file_count = 0\n",
        "  for f in files:\n",
        "    if file_count < val_size:\n",
        "      shutil.move(f'{files_path}/{f}', path_train)\n",
        "      file_count += 1\n",
        "    else:\n",
        "      shutil.move(f'{files_path}/{f}', path_val)\n",
        "      file_count += 1\n",
        "\n",
        "  print(f'Train set contains: {len(os.listdir(path_train))} files')\n",
        "  print(f'Val set contains: {len(os.listdir(path_val))} files')\n",
        "\n",
        "class_names = ['W','B','R','G','U']\n",
        "for name in class_names:\n",
        "   val_train_maker(name, 120)"
      ],
      "metadata": {
        "id": "JO9NwUSJdB2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33c6db75-c921-41d8-854e-c48431402393"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully created the directories\n",
            "Train set contains: 120 files\n",
            "Val set contains: 480 files\n",
            "Successfully created the directories\n",
            "Train set contains: 120 files\n",
            "Val set contains: 480 files\n",
            "Successfully created the directories\n",
            "Train set contains: 120 files\n",
            "Val set contains: 480 files\n",
            "Successfully created the directories\n",
            "Train set contains: 120 files\n",
            "Val set contains: 480 files\n",
            "Successfully created the directories\n",
            "Train set contains: 120 files\n",
            "Val set contains: 480 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading Data\n",
        "Nessa etapa será feita a proparação dos dados para serem alimentados no modelo"
      ],
      "metadata": {
        "id": "8PZnI81YrM9f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yuV1_FCETrT9"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function, division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformações aléatorias serão aplicadas aos dados para criar artificialmente um numero maior de imagens para o modelo"
      ],
      "metadata": {
        "id": "_Mq2GpuYQhJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make transforms and use data loaders\n",
        "\n",
        "# We'll use these a lot, so make them variables\n",
        "mean_nums = [0.485, 0.456, 0.406]\n",
        "std_nums = [0.229, 0.224, 0.225]\n",
        "\n",
        "chosen_transforms = {'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(size=256),\n",
        "        transforms.RandomRotation(degrees=15),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean_nums, std_nums)\n",
        "]), 'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean_nums, std_nums)\n",
        "]),\n",
        "}\n",
        "\n",
        "# Set the directory for the data\n",
        "data_dir = 'mtg_dataset/'\n",
        "# Use the image folder function to create datasets\n",
        "chosen_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), chosen_transforms[x]) for x in ['train', 'val']}"
      ],
      "metadata": {
        "id": "ikiFEf4HYOw3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalmente sao criados os elementos iteraveis com os dados"
      ],
      "metadata": {
        "id": "C65pauSgQ-Jf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make iterables with the dataloaders\n",
        "dataloaders = {x: torch.utils.data.DataLoader(chosen_datasets[x], batch_size=4,\n",
        "  shuffle=True, num_workers=4)\n",
        "              for x in ['train', 'val']}"
      ],
      "metadata": {
        "id": "X1FtbEvYYVtv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9d55ff7-60ba-46df-8476-fba212d63f19"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_sizes = {x: len(chosen_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = chosen_datasets['train'].classes"
      ],
      "metadata": {
        "id": "rd-huQLsYY77"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset_sizes)\n",
        "print(class_names)"
      ],
      "metadata": {
        "id": "FdCmeCTdYZjT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1369b13-2032-42b8-bfc1-375760c42f6a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train': 600, 'val': 2400}\n",
            "['B', 'G', 'R', 'U', 'W']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Modelo\n",
        "O modelo resnet utilizado será o não treinado, uma vez que o problema de classifcação de cartas de magic soa bastante distante do treinamento inicial da rede."
      ],
      "metadata": {
        "id": "_YKFSx57YgXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "    torch.cuda.is_available(),\n",
        "    torch.cuda.current_device(),\n",
        "    torch.cuda.get_device_name(0),\n",
        "    sep='\\n'\n",
        ")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyqkcKii2W2q",
        "outputId": "4590a245-1c0c-4b1e-966e-ee342b8c9e66"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "0\n",
            "Tesla K80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Será utilizada uma camada com 5 saídas no fim da rede, que deverão corresponder as categorias do dataset"
      ],
      "metadata": {
        "id": "5rnzcBl7RRuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the model\n",
        "# load in pretrained and reset final fully connected\n",
        "res_mod = models.resnet34(pretrained=False)\n",
        "num_ftrs = res_mod.fc.in_features\n",
        "res_mod.fc = nn.Linear(num_ftrs, 5)"
      ],
      "metadata": {
        "id": "XVhdnthWYb_N"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nesta etapa são setados alguns dos hyperparâmetros da rede, como taca de aprendizado"
      ],
      "metadata": {
        "id": "BvnCNrtRRd_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res_mod = res_mod.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(res_mod.parameters(), lr=0.0001, momentum=0.7)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ],
      "metadata": {
        "id": "KdKUGBmvZFn5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rotina de treino da rede"
      ],
      "metadata": {
        "id": "WLKyBHC-Rm3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=10):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            current_loss = 0.0\n",
        "            current_corrects = 0\n",
        "\n",
        "            # Here's where the training happens\n",
        "            print('Iterating through data...')\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # We need to zero the gradients, don't forget it\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Time to carry out the forward training poss\n",
        "                # We only need to log the loss stats if we are in training phase\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # We want variables to hold the loss statistics\n",
        "                current_loss += loss.item() * inputs.size(0)\n",
        "                current_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = current_loss / dataset_sizes[phase]\n",
        "            epoch_acc = current_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # Make a copy of the model if the accuracy on the validation set has improved\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_since = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_since // 60, time_since % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # Now we'll load in the best model weights and return it\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "metadata": {
        "id": "VEFwwlmEwE5F"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize some images\n",
        "def imshow(inp, title=None):\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([mean_nums])\n",
        "    std = np.array([std_nums])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # Pause a bit so that plots are updated"
      ],
      "metadata": {
        "id": "av7KVTrO6CFq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_handeled = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_handeled += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_handeled)\n",
        "                ax.axis('off')\n",
        "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_handeled == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)"
      ],
      "metadata": {
        "id": "k1RFJ2UfwIh7"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = train_model(res_mod, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=20)\n",
        "visualize_model(base_model)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OONxNMWMwMl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusões\n",
        "A rede resnet não se mostrou muito eficiente na classificação de cartas de Magic nas circunstâncias aqui apresentadas. \n",
        "\n",
        "Durante o processo de treinamento, foram testados diferentes \"lr\" e momentos alem de uma maior quantidade de epocas.\n",
        "\n",
        "O modelo estabilizou por volta de 14 a 16 epocas a depender dos parâmetros empregados.\n",
        "\n",
        "Uma possibilidade que poderá ser explorada é a de aumentar a quantidade de elementos do dataset\n",
        ".\n"
      ],
      "metadata": {
        "id": "H3XhSVhYUkcD"
      }
    }
  ]
}